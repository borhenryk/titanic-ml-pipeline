# Titanic ML Pipeline

End-to-end Machine Learning pipeline for Titanic survival prediction using Databricks Asset Bundles (DABs) with automated CI/CD deployment via GitHub Actions.

## ğŸš€ Quick Start

### Prerequisites
- Databricks workspace with Unity Catalog enabled
- Databricks CLI configured
- GitHub account with repository secrets configured

### Deploy to Development
```bash
cd titanic_ml_project
databricks bundle validate -t dev
databricks bundle deploy -t dev
```

### Run the Pipeline
```bash
databricks bundle run titanic_ml_pipeline -t dev
```

## ğŸ“ Project Structure

```
titanic_ml_project/
â”œâ”€â”€ databricks.yml              # Bundle configuration
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ titanic_pipeline_job.yml  # Job definition
â”œâ”€â”€ src/titanic_ml/notebooks/
â”‚   â”œâ”€â”€ 01_data_preparation.py    # Load & prepare data
â”‚   â”œâ”€â”€ 02_feature_engineering.py # Feature creation
â”‚   â”œâ”€â”€ 03_model_training.py      # Train with HPO
â”‚   â”œâ”€â”€ 04_model_validation.py    # Validate model
â”‚   â””â”€â”€ 05_model_deployment.py    # Deploy endpoint
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ eda_report.md             # EDA findings
â””â”€â”€ .github/workflows/
    â””â”€â”€ databricks-deploy.yml     # CI/CD pipeline
```

## ğŸ”§ Configuration

### Bundle Variables
| Variable | Description | Default |
|----------|-------------|---------|
| `catalog` | Unity Catalog name | `dbdemos_henryk` |
| `schema` | Schema for ML artifacts | `titanic_ml` |
| `experiment_name` | MLflow experiment path | `/Shared/titanic_ml_experiment` |

### Deployment Targets
- **dev**: Development environment (default)
- **staging**: Pre-production testing
- **prod**: Production deployment

## ğŸ”„ CI/CD Pipeline

The GitHub Actions workflow automatically:

1. **On Push to `main`/`develop`**: Validates and deploys to dev
2. **On Push to `main`**: Promotes through staging â†’ prod
3. **On Pull Request**: Validates bundle configuration
4. **Manual Trigger**: Deploy to any environment

### Required Secrets
- `DATABRICKS_HOST`: Workspace URL
- `DATABRICKS_TOKEN`: Personal Access Token

## ğŸ“Š Pipeline Stages

| Stage | Description | Output |
|-------|-------------|--------|
| Data Preparation | Load Titanic dataset, quality checks | `titanic_raw` table |
| Feature Engineering | Create ML features | `titanic_features` table |
| Model Training | Hyperparameter optimization with Optuna | Registered model |
| Model Validation | Performance validation against thresholds | Validation report |
| Model Deployment | Deploy to serving endpoint | REST API endpoint |

## ğŸ¯ Model Performance

Target metrics for deployment approval:
- Accuracy: â‰¥ 75%
- Precision: â‰¥ 70%
- Recall: â‰¥ 50%
- F1 Score: â‰¥ 60%
- ROC AUC: â‰¥ 75%

## ğŸ“ˆ MLflow Tracking

All experiments are logged to MLflow with:
- Hyperparameters
- Performance metrics
- Model artifacts
- Feature importance plots
- Confusion matrices

## ğŸ”— Resources

- [Databricks Asset Bundles Documentation](https://docs.databricks.com/dev-tools/bundles/index.html)
- [MLflow on Databricks](https://docs.databricks.com/mlflow/index.html)
- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html)

## ğŸ“ License

This project is for demonstration purposes.

---

*Last updated: December 16, 2025 - CI/CD pipeline ready* âœ…
